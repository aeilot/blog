<!DOCTYPE html>
<html lang="en-US">
  <head prefix="og: https://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="color-scheme" content="light dark">
  
  <title>CS231n Lecture Note II: Linear Classifiers - Louis C Deng&#39;s Blog</title>
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
  <link rel='manifest' href='/manifest.json'>
  

  <meta name="description" content="With the disadvantages of the KNN algorithm, we need to come up with a more powerful approach. The new approach will have two major components: a score function that maps the raw data to class scores,">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231n Lecture Note II: Linear Classifiers">
<meta property="og:url" content="https://blog.aeilot.top/2026/02/10/cs231n-lecture-ii/index.html">
<meta property="og:site_name" content="Louis C Deng&#39;s Blog">
<meta property="og:description" content="With the disadvantages of the KNN algorithm, we need to come up with a more powerful approach. The new approach will have two major components: a score function that maps the raw data to class scores,">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cs231n.github.io/assets/wb.jpeg">
<meta property="og:image" content="https://cs231n.github.io/assets/margin.jpg">
<meta property="og:image" content="https://cs231n.github.io/assets/svmvssoftmax.png">
<meta property="article:published_time" content="2026-02-11T00:45:09.000Z">
<meta property="article:modified_time" content="2026-02-10T19:11:09.768Z">
<meta property="article:author" content="Louis C Deng">
<meta property="article:tag" content="CS231n">
<meta property="article:tag" content="Computer Vision">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cs231n.github.io/assets/wb.jpeg">

  <script>window.ThemeCupertino = {}</script>

  <link rel="preconnect" href="https://rsms.me/">
  <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
  
  
<link rel="stylesheet" href="https://unpkg.com/simple-icons-font@16/font/simple-icons.min.css">

  
  
  
<link rel="stylesheet" href="https://unpkg.com/bootstrap-icons@1/font/bootstrap-icons.min.css">

  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">


  
  
<link rel="stylesheet" href="/css/components/card.css">

  
  
  
<link rel="stylesheet" href="/css/components/button.css">

  
  
  
<link rel="stylesheet" href="/css/components/badge.css">

  
  
  
<link rel="stylesheet" href="/css/components/utilities.css">

  
  
  
<link rel="stylesheet" href="/css/components/carousel.css">

  
  
  <script>ThemeCupertino['_calloutStyles'] = `
<link rel="stylesheet" href="/css/components/callout.css">
`</script>
  
<script src="/js/components/callout.js"></script>

  

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
<link rel="stylesheet" href="/css/rainbow-banner.css">

  
  
  
<link rel="stylesheet" href="/css/toc.css">

  
  
  
  
<link rel="stylesheet" href="/css/post.css">

  
  
  
  
  
  
  
<link rel="stylesheet" href="/css/scroll-reveal.css">

  
  
  
<link rel="stylesheet" href="/css/view-transition.css">

  
  

  
<script src="/js/head.js"></script>

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 8.1.1"></head>

  <body
    data-color-scheme="auto"
    data-uppercase-categories="true"
    
    data-rainbow-banner="true"
    data-rainbow-banner-shown="auto"
    data-rainbow-banner-month="6"
    data-rainbow-banner-colors="#e50000,#ff8d00,#ffee00,#008121,#004cff,#760188"
    
    data-config-root="/"
    
    data-toc="true"
    data-toc-max-depth="2"
    
    
    
    data-scroll-reveal-disappear="false"
    data-scroll-reveal-query=".scroll-reveal, .post-list-item, .card, .content p img, .content .block-large img"
    
    data-nav-blur-gradient="true"
    style="--config-corner-squircle: true"
  >
    
<script src="/js/body-top.js"></script>

    <a href="#main-content" id="skip-to-content">Skip to content</a>
    

<div class="nav-mask">
  
  <div style="--sharp: 1"></div>
  
  <div style="--sharp: 2"></div>
  
  <div style="--sharp: 3"></div>
  
  <div style="--sharp: 4"></div>
  
  <div style="--sharp: 5"></div>
  
  <div style="--sharp: 6"></div>
  
</div>

<nav id="theme-nav">
  <div class="inner">
    <a class="title" href="/">Blog</a>
    <div class="nav-arrow"></div>
    <div class="nav-items">
      <a class="nav-item nav-item-home" href="/" style="--index: 0">Home</a>

      
      <a class="nav-item" target="_blank" rel="noopener" href="https://aeilot.top/" style="--index: 1">Portfolio</a>
      
      <a class="nav-item" href="/archives" style="--index: 2">Archives</a>
      
      <a class="nav-item" href="/about" style="--index: 3">About</a>
      
      <a class="nav-item is-icon" href="/search" style="--index: 4"><i class="bi bi-search"></i></a>
      
    </div>
  </div>
</nav>

    <main id="main-content" data-pagefind-body>
      
<article class="post">
  
  <div class="meta">
    
    <div class="categories text-uppercase">
    
      <a href="/categories/EN-CS231n/">EN | CS231n</a>
    
    </div>
    

    
    <time class="date" datetime="2026-02-10T19:45:09-05:00">
      February 10, 2026
    </time>
    

    <h1 class="title" data-pagefind-meta="title">CS231n Lecture Note II: Linear Classifiers</h1>
  </div>

  <div class="content">
    <p>With the disadvantages of the KNN algorithm, we need to come up with a more powerful approach. The new approach will have two major components: a <strong>score function</strong> that maps the raw data to class scores, and a <strong>loss function</strong> that quantifies the agreement between the predicted scores and the ground truth labels.</p>
<span id="more"></span>
<h2 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h2><p>The score function maps the pixel values of an image to confidence scores for each class.</p>
<p>As before, let’s assume a training dataset of images $\mathbf{x}_i \in \mathbf{R}^D$, each associated with a label $y_i$. Here $i = 1 \dots N$ and $y_i \in 1 \dots K$.</p>
<p>That is, we have $\mathbf{N}$ examples (each with a dimensionality $\mathbf{D}$) and $\mathbf{K}$ distinct categories.</p>
<p>We will define the score function $f : \mathbf{R}^D \mapsto \mathbf{R}^K$ that maps the raw image pixels to class scores.</p>
<h2 id="Linear-Classifier"><a href="#Linear-Classifier" class="headerlink" title="Linear Classifier"></a>Linear Classifier</h2><p>We will start out with arguably the simplest possible function, a linear mapping.</p>
<script type="math/tex; mode=display">f(\mathbf{x}_i, \mathbf{W}, \mathbf{b}) = \mathbf{W}\mathbf{x}_i + \mathbf{b}</script><p>In the above equation, we are assuming that the image $\mathbf{x}_i$ has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix $\mathbf{W}$ (of size [K x D]), and the vector $\mathbf{b}$ (of size [K x 1]) are the <strong>parameters</strong> of the function.</p>
<p>The parameters in $\mathbf{W}$ are often called the weights, and $\mathbf{b}$ is called the bias vector because it influences the output scores, but without interacting with the actual data $\mathbf{x}_i$.</p>
<p>The input data are given and fixed. The goal is to set $\mathbf{W,b}$ in such way that the computed scores match the ground truth labels across the whole training set.</p>
<h3 id="Bias-Tricks"><a href="#Bias-Tricks" class="headerlink" title="Bias Tricks"></a>Bias Tricks</h3><p>We can combine the two sets of parameters into a single matrix that holds both of them by extending the vector $\mathbf{x}_i$ with one additional dimension that always holds the constant $\mathbf{1}$ - a default bias dimension.</p>
<script type="math/tex; mode=display">f(\mathbf{x}_i, \mathbf{W}) = \mathbf{W}\mathbf{x}_i</script><p><img src="https://cs231n.github.io/assets/wb.jpeg" alt="Bias Tricks"></p>
<h3 id="Image-Data-Preprocessing"><a href="#Image-Data-Preprocessing" class="headerlink" title="Image Data Preprocessing"></a>Image Data Preprocessing</h3><p>In Machine Learning, it is a very common practice to always perform normalization of input features. In particular, it is important to <strong>center your data</strong> by subtracting the mean from every feature.</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>We will develop <strong>Multiclass Support Vector Machine</strong> (SVM) loss.</p>
<p>The score function takes the pixels and computes the vector $f(\mathbf{x}_i, \mathbf{W})$ of class scores, which we will abbreviate to $\mathbf{s}$ (short for scores).</p>
<p>The Multiclass SVM loss for the i-th example is then formalized as follows:</p>
<script type="math/tex; mode=display">L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)</script><p>The function accumulates the error of incorrect classes within Delta.</p>
<p>In summary, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by at least by $\Delta$ (delta).</p>
<p>The threshold at zero max(0, -) function is often called the <strong>hinge loss</strong>. We also have squared hinge loss SVM (or L2-SVM), which uses the form max(0, -)² that penalizes violated margins more strongly.</p>
<p><img src="https://cs231n.github.io/assets/margin.jpg" alt="The Multiclass Support Vector Machine &quot;wants&quot; the score of the correct class to be higher than all other scores by at least a margin of delta."></p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>We wish to encode some preference for a certain set of weights W over others to remove this ambiguity. We can do so by extending the loss function with a regularization penalty $R(\mathbf{W})$. The most common regularization penalty is the squared L2 norm that discourages large weights through an elementwise quadratic penalty over all parameters:</p>
<script type="math/tex; mode=display">R(\mathbf{W}) = \sum_k \sum_l W_{k,l}^2</script><p>Including the regularization penalty completes the full Multiclass Support Vector Machine loss, which is made up of two components: the data loss (which is the average loss $L_i$ over all examples) and the regularization loss.</p>
<script type="math/tex; mode=display">L = \underbrace{\frac{1}{N} \sum_i L_i}_{\text{data loss}} + \underbrace{\lambda R(\mathbf{W})}_{\text{regularization loss}}</script><p>Or in full form:</p>
<script type="math/tex; mode=display">L = \frac{1}{N} \sum_i \sum_{j \neq y_i} \left[ \max(0, f(\mathbf{x}_i; \mathbf{W})_j - f(\mathbf{x}_i; \mathbf{W})_{y_i} + \Delta) \right] + \lambda \sum_k \sum_l W_{k,l}^2</script><p>Penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. It keeps the weights small and simple. This can improve the generalization performance of the classifiers on test images and lead to less overfitting.</p>
<p>It prevents the model from doing <em>too well</em> on the training data.</p>
<p>Note that due to the regularization penalty we can never achieve loss of exactly 0.0 on all examples.</p>
<h3 id="Practical-Considerations"><a href="#Practical-Considerations" class="headerlink" title="Practical Considerations"></a>Practical Considerations</h3><p><strong>Setting Delta</strong>: It turns out that this hyperparameter can safely be set to $\Delta = 1.0$ in all cases. (The exact value of the margin between the scores is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily.)</p>
<p><strong>Binary Support Vector Machine</strong>: The loss for the i-th example can be written as</p>
<script type="math/tex; mode=display">L_i = C \max(0, 1 - y_i \mathbf{w}^T \mathbf{x}_i) + R(\mathbf{W})</script><p>$\mathbf{C}$ in this formulation and $\lambda$ in our formulation control the same tradeoff and are related through reciprocal relation $C \propto \frac{1}{\lambda}$.</p>
<p><strong>Other Multiclass SVM formulations</strong>: Multiclass SVM presented in this section is one of few ways of formulating the SVM over multiple classes.</p>
<p>Another commonly used form is the One-Vs-All (OVA) SVM which trains an independent binary SVM for each class vs. all other classes. Related, but less common to see in practice is also the All-vs-All (AVA) strategy.</p>
<p>The last formulation you may see is a Structured SVM, which maximizes the margin between the score of the correct class and the score of the highest-scoring incorrect runner-up class.</p>
<h2 id="Softmax-Classifier"><a href="#Softmax-Classifier" class="headerlink" title="Softmax Classifier"></a>Softmax Classifier</h2><p>In the Softmax Classifier, we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form:</p>
<script type="math/tex; mode=display">L_i = -\log\left(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}\right) \hspace{1cm} \text{or equivalently} \hspace{1cm} L_i = -f_{y_i} + \log\sum_j e^{f_j}</script><p>where we are using the notation $f_j$ to mean the j-th element of the vector of class scores $\mathbf{f}$. </p>
<p>The function $f_j(\mathbf{z}) = \frac{e^{z_j}}{\sum_k e^{z_k}}$ is called the softmax function: It takes a vector of arbitrary real-valued scores (in $\mathbf{z}$) and squashes it to a vector of values between zero and one that sum to one. </p>
<h3 id="Information-Theory-View"><a href="#Information-Theory-View" class="headerlink" title="Information Theory View"></a>Information Theory View</h3><p>The cross-entropy between a “true” distribution $\mathbf{p}$ and an estimated distribution $\mathbf{q}$ is defined as:</p>
<script type="math/tex; mode=display">H(\mathbf{p}, \mathbf{q}) = -\sum_x p(x) \log q(x)</script><p>Minimizing Cross-Entropy is equivalent to minimizing the KL Divergence.</p>
<script type="math/tex; mode=display">H(p, q) = H(p) + D_{KL}(p||q)</script><p>Because the true distribution $p$ is fixed (its entropy $H(p)$ is zero in this scenario), minimizing cross-entropy is the same as forcing the predicted distribution $q$ to look exactly like the true distribution $p$.</p>
<p>The Softmax Loss objective is to force the neural network to output a probability distribution where the correct class has a probability very close to 1.0, and all other classes are close to 0.0.</p>
<h3 id="Information-Theory-Supplementary"><a href="#Information-Theory-Supplementary" class="headerlink" title="Information Theory Supplementary"></a>Information Theory Supplementary</h3><p>Information Entropy measures the uncertainty or unpredictability of a random variable.</p>
<p>The more unpredictable an event is (lower probability), the more information is gained when it occurs, and the higher the entropy. Conversely, if an event has a probability of 1 (certainty), its entropy is 0.</p>
<p>For a discrete random variable $X$ with possible outcomes ${x_1, …, x_n}$ and probabilities $P(x_i)$, the entropy $H(X)$ is defined as:</p>
<script type="math/tex; mode=display">H(X) = -\sum_{i=1}^{n} P(x_i) \log P(x_i)</script><p>Cross Entropy measures the total cost of using distribution $q$ to represent distribution $p$.</p>
<p>Minimizing the cross-entropy $H(p, q)$ is mathematically equivalent to minimizing the KL Divergence. It forces the predicted distribution $q$ to become as close as possible to the true distribution $p$.</p>
<h3 id="Probabilistic-View"><a href="#Probabilistic-View" class="headerlink" title="Probabilistic View"></a>Probabilistic View</h3><script type="math/tex; mode=display">P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j}}</script><p>The formula maps raw scores to a range of $(0, 1)$ such that the sum of all class probabilities equals 1.</p>
<p>Using the Cross-Entropy loss function during training is equivalent to maximizing the likelihood of the correct class.</p>
<h3 id="Numeric-Stability"><a href="#Numeric-Stability" class="headerlink" title="Numeric Stability"></a>Numeric Stability</h3><p>Dividing large numbers can be numerically unstable, so it is important to use a normalization trick.</p>
<script type="math/tex; mode=display">\frac{e^{f_{y_i}}}{\sum_j e^{f_j}} = \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}} = \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}</script><p>A common choice for $C$ is to set $\log C = -\max_j f_j$. This simply states that we should shift the values inside the vector $f$ so that the highest value is zero. In code:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># example with 3 classes and each having large scores</span></span><br><span class="line">p = np.exp(f) / np.<span class="built_in">sum</span>(np.exp(f)) <span class="comment"># Bad: Numeric problem, potential blowup</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># instead: first shift the values of f so that the highest number is 0:</span></span><br><span class="line">f -= np.<span class="built_in">max</span>(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.<span class="built_in">sum</span>(np.exp(f)) <span class="comment"># safe to do, gives the correct answer</span></span><br></pre></td></tr></table></figure>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>To be precise, the SVM classifier uses the hinge loss, or also sometimes called the max-margin loss.</p>
<p>The Softmax classifier uses the cross-entropy loss.</p>
<h2 id="SVM-vs-Softmax"><a href="#SVM-vs-Softmax" class="headerlink" title="SVM vs Softmax"></a>SVM vs Softmax</h2><p><img src="https://cs231n.github.io/assets/svmvssoftmax.png" alt="SVM vs Softmax"></p>
<p>The SVM interprets these as class scores and its loss function encourages the correct class to have a score higher by a margin than the other class scores.</p>
<p>The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high (equivalently the negative of it to be low).</p>
<p>Softmax classifier provides “probabilities” for each class.</p>
<p>The “probabilities” are dependent on the regularization strength. They are better thought of as confidences where the ordering of the scores is interpretable.</p>
<p>In practice, SVM and Softmax are usually comparable. Compared to the Softmax classifier, the SVM is a more local objective. </p>
<p>The Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better.</p>
<p>However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint.</p>

  </div>

  
  <div class="about" data-pagefind-ignore>
    <h2>About this Post</h2>
    <div class="details">This post is written by Louis C Deng, licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0</a>.</div>
    
    <p class="tags">
      <i class="bi bi-tags icon"></i>
      
      <a href="/tags/CS231n/" class="tag">#CS231n</a>
      
      <a href="/tags/Deep-Learning/" class="tag">#Deep Learning</a>
      
      <a href="/tags/Computer-Vision/" class="tag">#Computer Vision</a>
      
    </p>
    
    
  </div>
  

  <div class="container post-prev-next" data-pagefind-ignore>
    
    <a href="/2026/02/10/cs231n-lecture-iii/" class="next">
      <div class="text">
        <p class="label">Next</p>
        <h3 class="title">CS231n Lecture Note III: Optimization</h3>
      </div>
    </a>
    
    
    <a href="/2026/02/09/cs231n-lecture-i/" class="prev">
      <div class="text">
        <p class="label">Previous</p>
        <h3 class="title">CS231n Lecture Note I: Image Classification</h3>
      </div>
    </a>
    
  </div>

  
    <script src="https://giscus.app/client.js"
        data-repo="aeilot/blog"
        data-repo-id="MDEwOlJlcG9zaXRvcnkyNjEwODA2OTY="
        data-category="General"
        data-category-id="DIC_kwDOD4_GeM4B_VIn"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

  
</article>


    </main>
    <footer>
  <div class="inner">
    <div class="links">
      
      <div class="group">
        <h2 class="title">Blog</h2>
        
        <a href="/" class="item">Home</a>
        
        <a href="/archives" class="item">Archives</a>
        
        <a href="/tags" class="item">Tags</a>
        
        <a href="/categories" class="item">Categories</a>
        
        <a href="/search" class="item">Search</a>
        
        <a href="/index.xml" class="item">RSS</a>
        
      </div>
      
      <div class="group">
        <h2 class="title">Links</h2>
        
        <a target="_blank" rel="noopener" href="https://aeilot.top" class="item">Portfolio</a>
        
        <a target="_blank" rel="noopener" href="https://github.com/aeilot" class="item">GitHub</a>
        
        <a target="_blank" rel="noopener" href="https://www.instagram.com/aeilotd/" class="item">Instagram</a>
        
        <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/chenluo-d-85b37a33a/" class="item">LinkedIn</a>
        
      </div>
      
      <div class="group">
        <h2 class="title">Friends</h2>
        
        <a target="_blank" rel="noopener" href="https://mrwillcom.now.sh" class="item">Mr.Will</a>
        
        <a target="_blank" rel="noopener" href="https://chungzh.cn" class="item">ChungZH</a>
        
        <a target="_blank" rel="noopener" href="https://shaojiaxu.github.io" class="item">SJX</a>
        
        <a target="_blank" rel="noopener" href="http://clckblog.space/" class="item">CLCK</a>
        
        <a target="_blank" rel="noopener" href="https://innei.in/" class="item">静かな森</a>
        
      </div>
      
    </div>
    <span>&copy; 2026 Louis C Deng<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></span>
    
    
      <br>
      <div class="color-scheme-toggle" role="radiogroup" id="theme-color-scheme-toggle">
        <label>
          <input type="radio" value="light">
          <span>Light</span>
        </label>
        <label>
          <input type="radio" value="dark">
          <span>Dark</span>
        </label>
        <label>
          <input type="radio" value="auto">
          <span>Auto</span>
        </label>
      </div>
    
  </div>
</footer>


    
<script src="/js/main.js"></script>


    

    
    
<script src="/js/scroll-reveal.js"></script>

    

    <style>
  .giscus {
    max-width: 768px;
    padding: 16px;
    margin: 64px auto;
    
    background: var(--color-background-secondary); 
    border-radius: var(--radius-medium); 
  }


  @media (max-width: 800px) {
      .giscus {
          border-radius: 0;
          width: auto;
      }
  }

  @container document-body style(--config-corner-squircle: true) {
      @supports (corner-shape: squircle) {
          .giscus {
              corner-shape: squircle;
              --radius-tiny: 10px;
              --radius-small: 20px;
              --radius-medium: 30px;
              --radius-large: 40px;
              --radius-full: 999px;
          }
      }
  }

  @media (max-width: 768px) {
    .giscus {
      margin: 20px 10px; 
      padding: 20px;
    }
  }
</style>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
