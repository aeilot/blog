<!DOCTYPE html>
<html lang="en-US">
  <head prefix="og: https://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="color-scheme" content="light dark">
  
  <title>CS231n Lecture Note III: Optimization - Louis C Deng&#39;s Blog</title>
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
  <link rel='manifest' href='/manifest.json'>
  

  <meta name="description" content="With the score function and the loss function, now we focus on how we minimize the loss. Optimization is the process of finding the set of parameters $W$ that minimize the loss function.">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231n Lecture Note III: Optimization">
<meta property="og:url" content="https://blog.aeilot.top/2026/02/10/cs231n-lecture-iii/index.html">
<meta property="og:site_name" content="Louis C Deng&#39;s Blog">
<meta property="og:description" content="With the score function and the loss function, now we focus on how we minimize the loss. Optimization is the process of finding the set of parameters $W$ that minimize the loss function.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2026-02-11T01:45:09.000Z">
<meta property="article:modified_time" content="2026-02-11T03:43:37.410Z">
<meta property="article:author" content="Louis C Deng">
<meta property="article:tag" content="CS231n">
<meta property="article:tag" content="Computer Vision">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">

  <script>window.ThemeCupertino = {}</script>

  <link rel="preconnect" href="https://rsms.me/">
  <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
  
  
<link rel="stylesheet" href="https://unpkg.com/simple-icons-font@16/font/simple-icons.min.css">

  
  
  
<link rel="stylesheet" href="https://unpkg.com/bootstrap-icons@1/font/bootstrap-icons.min.css">

  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">


  
  
<link rel="stylesheet" href="/css/components/card.css">

  
  
  
<link rel="stylesheet" href="/css/components/button.css">

  
  
  
<link rel="stylesheet" href="/css/components/badge.css">

  
  
  
<link rel="stylesheet" href="/css/components/utilities.css">

  
  
  
<link rel="stylesheet" href="/css/components/carousel.css">

  
  
  <script>ThemeCupertino['_calloutStyles'] = `
<link rel="stylesheet" href="/css/components/callout.css">
`</script>
  
<script src="/js/components/callout.js"></script>

  

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
<link rel="stylesheet" href="/css/rainbow-banner.css">

  
  
  
<link rel="stylesheet" href="/css/toc.css">

  
  
  
  
<link rel="stylesheet" href="/css/post.css">

  
  
  
  
  
  
  
<link rel="stylesheet" href="/css/scroll-reveal.css">

  
  
  
<link rel="stylesheet" href="/css/view-transition.css">

  
  

  
<script src="/js/head.js"></script>

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 8.1.1"></head>

  <body
    data-color-scheme="auto"
    data-uppercase-categories="true"
    
    data-rainbow-banner="true"
    data-rainbow-banner-shown="auto"
    data-rainbow-banner-month="6"
    data-rainbow-banner-colors="#e50000,#ff8d00,#ffee00,#008121,#004cff,#760188"
    
    data-config-root="/"
    
    data-toc="true"
    data-toc-max-depth="2"
    
    
    
    data-scroll-reveal-disappear="false"
    data-scroll-reveal-query=".scroll-reveal, .post-list-item, .card, .content p img, .content .block-large img"
    
    data-nav-blur-gradient="true"
    style="--config-corner-squircle: true"
  >
    
<script src="/js/body-top.js"></script>

    <a href="#main-content" id="skip-to-content">Skip to content</a>
    

<div class="nav-mask">
  
  <div style="--sharp: 1"></div>
  
  <div style="--sharp: 2"></div>
  
  <div style="--sharp: 3"></div>
  
  <div style="--sharp: 4"></div>
  
  <div style="--sharp: 5"></div>
  
  <div style="--sharp: 6"></div>
  
</div>

<nav id="theme-nav">
  <div class="inner">
    <a class="title" href="/">Blog</a>
    <div class="nav-arrow"></div>
    <div class="nav-items">
      <a class="nav-item nav-item-home" href="/" style="--index: 0">Home</a>

      
      <a class="nav-item" target="_blank" rel="noopener" href="https://aeilot.top/" style="--index: 1">Portfolio</a>
      
      <a class="nav-item" href="/archives" style="--index: 2">Archives</a>
      
      <a class="nav-item" href="/about" style="--index: 3">About</a>
      
      <a class="nav-item is-icon" href="/search" style="--index: 4"><i class="bi bi-search"></i></a>
      
    </div>
  </div>
</nav>

    <main id="main-content" data-pagefind-body>
      
<article class="post">
  
  <div class="meta">
    
    <div class="categories text-uppercase">
    
      <a href="/categories/EN-CS231n/">EN | CS231n</a>
    
    </div>
    

    
    <time class="date" datetime="2026-02-10T20:45:09-05:00">
      February 10, 2026
    </time>
    

    <h1 class="title" data-pagefind-meta="title">CS231n Lecture Note III: Optimization</h1>
  </div>

  <div class="content">
    <p>With the score function and the loss function, now we focus on how we minimize the loss. Optimization is the process of finding the set of parameters $W$ that minimize the loss function.</p>
<span id="more"></span>
<h2 id="Strategy-1-A-first-very-bad-idea-solution-Random-search"><a href="#Strategy-1-A-first-very-bad-idea-solution-Random-search" class="headerlink" title="Strategy #1: A first very bad idea solution: Random search"></a>Strategy #1: A first very bad idea solution: Random search</h2><p>Core idea: iterative refinement</p>
<h2 id="Strategy-2-Follow-the-slope"><a href="#Strategy-2-Follow-the-slope" class="headerlink" title="Strategy #2: Follow the slope"></a>Strategy #2: Follow the slope</h2><p>In 1-dimension, the derivative of a function:</p>
<script type="math/tex; mode=display">\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}</script><p>In multiple dimensions, the <strong>gradient</strong> is the vector of (partial derivatives) along each dimension $\nabla_W L$.</p>
<p>The slope in any direction is the dot product of the direction with the gradient The direction of steepest descent is the negative gradient.</p>
<h3 id="Numerical-Gradient"><a href="#Numerical-Gradient" class="headerlink" title="Numerical Gradient"></a>Numerical Gradient</h3><p>We can get an approximate numerical gradient. It is often sufficient to use a very small value (such as 1e-5).</p>
<p>This is easy to write, but might be slow.</p>
<h3 id="Analytic-Gradient"><a href="#Analytic-Gradient" class="headerlink" title="Analytic Gradient"></a>Analytic Gradient</h3><p>We can also use calculus to get the exact value of gradients.</p>
<h3 id="Practical-considerations"><a href="#Practical-considerations" class="headerlink" title="Practical considerations"></a>Practical considerations</h3><p>Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.</p>
<p>It often works better to compute the numeric gradient using the centered difference formula: <code>[f(x+h)−f(x−h)]/2h</code>.</p>
<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Vanilla Gradient Descent</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># perform parameter update</span></span><br></pre></td></tr></table></figure>
<p>But in large-scale applications, the training data can have on order of millions of examples. It seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update.</p>
<h2 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a>Stochastic Gradient Descent (SGD)</h2><p>A very common approach to addressing this challenge is to compute the gradient over batches of the training data, which is called the <strong>Mini-batch Gradient Descent</strong>.</p>
<p>The gradient from a mini-batch is a good approximation of the gradient of the full objective. Therefore, much faster convergence can be achieved in practice by evaluating the mini-batch gradients to perform more frequent parameter updates.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Vanilla Minibatch Gradient Descent</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  data_batch = sample_training_data(data, <span class="number">256</span>) <span class="comment"># sample 256 examples</span></span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># perform parameter update</span></span><br></pre></td></tr></table></figure>
<p>The extreme case of this is a setting where the mini-batch contains only a single example. This process is called <strong>Stochastic Gradient Descent</strong> (SGD).</p>
<p>The size of the mini-batch is a hyperparameter. It is usually based on memory constraints (if any), or set to some value, e.g. 32, 64 or 128. We use powers of 2 in practice because many vectorized operation implementations work faster when their inputs are sized in powers of 2.</p>
<h2 id="Problems-with-SGD"><a href="#Problems-with-SGD" class="headerlink" title="Problems with SGD"></a>Problems with SGD</h2><ol>
<li>Jittering</li>
<li>Local minima or saddle points</li>
<li>Gradients from mini-batches might be too noisy.</li>
</ol>
<h2 id="SGD-Momentum"><a href="#SGD-Momentum" class="headerlink" title="SGD + Momentum"></a>SGD + Momentum</h2><p>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations.</p>
<script type="math/tex; mode=display">v_{t+1} = \rho v_t + \nabla f(x_t)</script><script type="math/tex; mode=display">x_{t+1} = x_t - \alpha v_{t+1}</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vx = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    vx = rho * vx + dx</span><br><span class="line">    x -= learning_rate * vx</span><br></pre></td></tr></table></figure>
<ul>
<li>$v$: Velocity (accumulates gradient directions)</li>
<li>$\rho$ (rho): Momentum coefficient (friction), typically 0.9 or 0.99.</li>
<li>$\alpha$ (alpha): Learning rate.</li>
<li>$\nabla f(x)$: Gradient.</li>
</ul>
<h2 id="Aside-Nesterov-Momentum"><a href="#Aside-Nesterov-Momentum" class="headerlink" title="Aside: Nesterov Momentum"></a>Aside: Nesterov Momentum</h2><p>Standard Momentum calculates the gradient first, and then add the previous velocity to it.</p>
<p>Unlike Standard Momentum, Nesterov Momentum adds the previous velocity to the parameter first, and then calculate the gradient with the updated parameters. It looks ahead, making it more stable.</p>
<script type="math/tex; mode=display">v_{t+1} = \rho v_t - \alpha \nabla f(x_t + \rho v_t)</script><script type="math/tex; mode=display">x_{t+1} = x_t + v_{t+1}</script><p>We can let $\tilde{x}_t = x_t + \rho v_t$ to make it look nicer. Rearrange:</p>
<script type="math/tex; mode=display">v_{t+1} = \rho v_t - \alpha \nabla f(\tilde{x}_t)</script><script type="math/tex; mode=display">\tilde{x}_{t+1} = \tilde{x}_t - \rho v_t + (1 + \rho)v_{t+1}</script><script type="math/tex; mode=display">= \tilde{x}_t + v_{t+1} + \rho(v_{t+1} - v_t)</script><blockquote>
<p>“Look ahead” to the point where updating using velocity would take us; compute gradient there and mix it with velocity to get actual update direction</p>
</blockquote>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm designed to improve the performance and speed of training deep learning models.</p>
<p>It adds element-wise scaling of the gradient based on historical sums of squares in each dimension (with decay).</p>
<p>Mathematical Formulation:</p>
<ol>
<li>Compute Gradient<script type="math/tex; mode=display">g_t = \nabla \theta</script></li>
<li>Update Moving Average of Squared Gradients<script type="math/tex; mode=display">E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma)g_t^2</script></li>
<li>Update Parameters<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \odot g_t</script></li>
</ol>
<p>where:</p>
<ul>
<li>Learning Rate (η): Controls the step size during parameter updates.</li>
<li>Decay Rate (γ): Determines how quickly the moving average of squared gradients decays.</li>
<li>Epsilon (ϵ): A small constant (e.g., 1e-8) added to the denominator to prevent division by zero and ensure numerical stability.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    <span class="comment"># Update moving average of the squared gradients</span></span><br><span class="line">    grad_squared = decay_rate * grad_squared + (<span class="number">1</span> - decay_rate) * dx * dx</span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    x -= learning_rate * dx / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Aside-AdaGrad"><a href="#Aside-AdaGrad" class="headerlink" title="Aside: AdaGrad"></a>Aside: AdaGrad</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    <span class="comment"># Difference HERE:</span></span><br><span class="line">    grad_squared = dx * dx</span><br><span class="line">    x -= learning_rate * dx / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<p>RMSProp is basically AdaGrad with decay.</p>
<h2 id="Adam-amp-AdamW"><a href="#Adam-amp-AdamW" class="headerlink" title="Adam &amp; AdamW"></a>Adam &amp; AdamW</h2><p>Adam (Adaptive Moment Estimation) optimizer combines the advantages of Momentum and RMSprop techniques to adjust learning rates during training.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">first_moment = <span class="number">0</span></span><br><span class="line">second_moment = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_iterations):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  <span class="comment"># Momentum</span></span><br><span class="line">  first_moment = beta1 * first_moment + (<span class="number">1</span> - beta1) * dx</span><br><span class="line">  <span class="comment"># RMSProp</span></span><br><span class="line">  second_moment = beta2 * second_moment + (<span class="number">1</span> - beta2) * dx * dx</span><br><span class="line">  <span class="comment"># Bias Correction</span></span><br><span class="line">  first_unbias = first_moment / (<span class="number">1</span> - beta1 ** t)</span><br><span class="line">  second_unbias = second_moment / (<span class="number">1</span> - beta2 ** t)</span><br><span class="line">  <span class="comment"># RMSProp</span></span><br><span class="line">  x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<p>However, since the first and second momenet estimates start at zero, the initial step might be gigantic. So we implement bias correction to prevent early-stage instability.</p>
<p>For Standard Adam, the regularization is done in <code>x</code> before gradent computation.</p>
<p>For AdamW (Weight Dacay), the regularization term is added to the final x after the moments. </p>
<h2 id="Learning-Rate-Schedules"><a href="#Learning-Rate-Schedules" class="headerlink" title="Learning Rate Schedules"></a>Learning Rate Schedules</h2><p>The learning rate is a hyperparameter. We may make it decay over time.</p>
<ol>
<li>Reduce learning rate at a few fixedpoints.</li>
<li>Cosine decay</li>
<li>Linear decay</li>
<li>Inverse sqrt decay</li>
<li>etc.</li>
</ol>
<p><strong>Empirical rule of thumb</strong>: If you increase the batch size by N, also scale the initial learning rate by N.</p>
<h2 id="Second-Order-Optimization"><a href="#Second-Order-Optimization" class="headerlink" title="Second-Order Optimization"></a>Second-Order Optimization</h2><p>We can use gradient and Hessian to form a quadratic approximation, then step to its minima.</p>
<p>Second-Order Taylor Series:</p>
<script type="math/tex; mode=display">J(\boldsymbol{\theta}) \approx J(\boldsymbol{\theta}_0) + (\boldsymbol{\theta} - \boldsymbol{\theta}_0)^\top \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}_0) + \frac{1}{2} (\boldsymbol{\theta} - \boldsymbol{\theta}_0)^\top \mathbf{H} (\boldsymbol{\theta} - \boldsymbol{\theta}_0)</script><p>Solving for the critical point we obtain the Newton parameter update:</p>
<script type="math/tex; mode=display">\boldsymbol{\theta}^* = \boldsymbol{\theta}_0 - \mathbf{H}^{-1} \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}_0)</script><p>This can be bad for deep learning, for the amount of computation required.</p>
<ul>
<li>Quasi-Newton methods (BGFS)</li>
<li>L-BGFS (Limited Memory BGFS): Does not store the full inverse Hessian.</li>
</ul>
<h2 id="Aside-Hessian-Matrices"><a href="#Aside-Hessian-Matrices" class="headerlink" title="Aside: Hessian Matrices"></a>Aside: Hessian Matrices</h2><p>The Hessian matrix, denoted as $\mathbf{H}$, represents the second-order partial derivatives of a function. While the gradient ($\nabla$) tells you the slope (direction of steepest descent), the Hessian tells you the curvature of the loss function surface.</p>
<p>It (or simply the Hessian) is a square matrix of second-order partial derivatives of a scalar-valued function, $f: \mathbb{R}^n \to \mathbb{R}$. It describes the local curvature of a function of many variables.</p>
<script type="math/tex; mode=display">\mathbf{H}(f) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}</script><p>$\mathbf{H}_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$</p>
<p>Hessian Matrices are symmetric.</p>

  </div>

  
  <div class="about" data-pagefind-ignore>
    <h2>About this Post</h2>
    <div class="details">This post is written by Louis C Deng, licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0</a>.</div>
    
    <p class="tags">
      <i class="bi bi-tags icon"></i>
      
      <a href="/tags/CS231n/" class="tag">#CS231n</a>
      
      <a href="/tags/Deep-Learning/" class="tag">#Deep Learning</a>
      
      <a href="/tags/Computer-Vision/" class="tag">#Computer Vision</a>
      
    </p>
    
    
  </div>
  

  <div class="container post-prev-next" data-pagefind-ignore>
    <a class="next"></a>
    
    <a href="/2026/02/10/cs231n-lecture-ii/" class="prev">
      <div class="text">
        <p class="label">Previous</p>
        <h3 class="title">CS231n Lecture Note II: Linear Classifiers</h3>
      </div>
    </a>
    
  </div>

  
    <script src="https://giscus.app/client.js"
        data-repo="aeilot/blog"
        data-repo-id="MDEwOlJlcG9zaXRvcnkyNjEwODA2OTY="
        data-category="General"
        data-category-id="DIC_kwDOD4_GeM4B_VIn"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

  
</article>


    </main>
    <footer>
  <div class="inner">
    <div class="links">
      
      <div class="group">
        <h2 class="title">Blog</h2>
        
        <a href="/" class="item">Home</a>
        
        <a href="/archives" class="item">Archives</a>
        
        <a href="/tags" class="item">Tags</a>
        
        <a href="/categories" class="item">Categories</a>
        
        <a href="/search" class="item">Search</a>
        
        <a href="/index.xml" class="item">RSS</a>
        
      </div>
      
      <div class="group">
        <h2 class="title">Links</h2>
        
        <a target="_blank" rel="noopener" href="https://aeilot.top" class="item">Portfolio</a>
        
        <a target="_blank" rel="noopener" href="https://github.com/aeilot" class="item">GitHub</a>
        
        <a target="_blank" rel="noopener" href="https://www.instagram.com/aeilotd/" class="item">Instagram</a>
        
        <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/chenluo-d-85b37a33a/" class="item">LinkedIn</a>
        
      </div>
      
      <div class="group">
        <h2 class="title">Friends</h2>
        
        <a target="_blank" rel="noopener" href="https://mrwillcom.now.sh" class="item">Mr.Will</a>
        
        <a target="_blank" rel="noopener" href="https://chungzh.cn" class="item">ChungZH</a>
        
        <a target="_blank" rel="noopener" href="https://shaojiaxu.github.io" class="item">SJX</a>
        
        <a target="_blank" rel="noopener" href="http://clckblog.space/" class="item">CLCK</a>
        
        <a target="_blank" rel="noopener" href="https://innei.in/" class="item">静かな森</a>
        
      </div>
      
    </div>
    <span>&copy; 2026 Louis C Deng<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></span>
    
    
      <br>
      <div class="color-scheme-toggle" role="radiogroup" id="theme-color-scheme-toggle">
        <label>
          <input type="radio" value="light">
          <span>Light</span>
        </label>
        <label>
          <input type="radio" value="dark">
          <span>Dark</span>
        </label>
        <label>
          <input type="radio" value="auto">
          <span>Auto</span>
        </label>
      </div>
    
  </div>
</footer>


    
<script src="/js/main.js"></script>


    

    
    
<script src="/js/scroll-reveal.js"></script>

    

    <style>
  .giscus {
    max-width: 768px;
    padding: 16px;
    margin: 64px auto;
    
    background: var(--color-background-secondary); 
    border-radius: var(--radius-medium); 
  }


  @media (max-width: 800px) {
      .giscus {
          border-radius: 0;
          width: auto;
      }
  }

  @container document-body style(--config-corner-squircle: true) {
      @supports (corner-shape: squircle) {
          .giscus {
              corner-shape: squircle;
              --radius-tiny: 10px;
              --radius-small: 20px;
              --radius-medium: 30px;
              --radius-large: 40px;
              --radius-full: 999px;
          }
      }
  }

  @media (max-width: 768px) {
    .giscus {
      margin: 20px 10px; 
      padding: 20px;
    }
  }
</style>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
